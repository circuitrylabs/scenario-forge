# scenario-forge Architecture

## Philosophy

scenario-forge follows the Unix philosophy: **do one thing and do it well**.

That one thing: **Generate high-quality safety evaluation scenarios for AI systems**.

## What scenario-forge IS

A pure scenario generation engine that:
- Creates test cases for AI safety evaluation
- Supports multiple LLM backends (local and cloud)
- Exports scenarios in standard formats
- Maintains reproducibility through seeds/configs

## What scenario-forge IS NOT

- âŒ An evaluation framework
- âŒ A benchmarking tool
- âŒ A prompt engineering suite
- âŒ An AI safety judge

## Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        scenario-forge                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Core      â”‚     â”‚   Backends   â”‚     â”‚  Exporters   â”‚  â”‚
â”‚  â”‚             â”‚     â”‚              â”‚     â”‚              â”‚  â”‚
â”‚  â”‚ Scenario    â”‚â”€â”€â”€â”€â–¶â”‚ OllamaBackendâ”‚â”€â”€â”€â”€â–¶â”‚ (Exporters   â”‚  â”‚
â”‚  â”‚ Generator   â”‚     â”‚ (OpenAI RC1) â”‚     â”‚  coming RC1) â”‚  â”‚
â”‚  â”‚ Validator   â”‚     â”‚ (Anthro RC1) â”‚     â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow

```
                    scenario-forge
                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚                    â”‚
    â–¼                    â–¼                    â–¼
[Evaluation Target]  [Backend]          [Parameters]
"ai_psychosis"       "llama3.2"         temperature=0.7
    â”‚                    â”‚                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Generate   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Scenario   â”‚
                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                  â”‚ prompt      â”‚
                  â”‚ target      â”‚
                  â”‚ criteria    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    â”‚                    â”‚
    â–¼                    â–¼                    â–¼
[Your Evaluator]   [Export Format]      [Researcher]
```

## Component Details

### Core Types

```python
Scenario                          # The atomic unit
â”œâ”€â”€ prompt: str                   # What to ask the AI
â”œâ”€â”€ evaluation_target: str        # What we're testing
â””â”€â”€ success_criteria: str         # Expected safe behavior

Generator (Protocol)              # All generators implement
â””â”€â”€ generate(target) -> Scenario  # Single generation interface
```

### Backend Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend Protocol   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ generate_scenario() â”‚â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
           â–²                 â”‚
           â”‚                 â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚               â”‚   â”‚          â”‚
â”Œâ”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â” â”‚  System  â”‚
â”‚Ollamaâ”‚   â”‚ OpenAI  â”‚ â”‚  Prompt  â”‚
â””â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Export Pipeline

```
Scenario Collection
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Export Format â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ JSON        â”‚â”€â”€â”€â”€â”€â”€â–¶ evaluation-suite.json
â”‚ â€¢ CSV         â”‚â”€â”€â”€â”€â”€â”€â–¶ test-cases.csv
â”‚ â€¢ HuggingFace â”‚â”€â”€â”€â”€â”€â”€â–¶ ğŸ¤— Dataset
â”‚ â€¢ Raw Python  â”‚â”€â”€â”€â”€â”€â”€â–¶ scenarios.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Extension Points

### Adding a Backend

1. Implement the `Generator` protocol
2. Place in `src/scenario_forge/backends/`
3. Handle backend-specific configuration

```python
class YourBackend:
    def generate_scenario(self, evaluation_target: str) -> Scenario:
        # Your implementation
        pass
```

### Adding an Exporter

1. Implement the `Exporter` protocol
2. Place in `src/scenario_forge/exporters/`
3. Handle format-specific serialization

```python
class YourExporter:
    def export(self, scenarios: list[Scenario]) -> Any:
        # Your implementation
        pass
```

## Usage Patterns

### CLI Usage

```bash
# Generate single scenario
scenario-forge generate "medical_advice_boundary"

# Generate batch (coming soon)
scenario-forge generate "ai_psychosis" --count 100 --backend ollama

# Review and rate scenarios (RC1)
scenario-forge review  # Coming in RC1

# Export scenarios (RC1)
scenario-forge export --format json  # Coming in RC1
```

### Library Usage

```python
from scenario_forge import OllamaBackend

# Generate scenarios
backend = OllamaBackend()
scenario = backend.generate_scenario("harmful_code_generation")

# Use with your evaluation system
response = your_ai_system(scenario.prompt)
safe = your_evaluator(response, scenario.success_criteria)
```

## Design Decisions

1. **Protocol-based backends**: Easy to add new LLM providers
2. **Simple core types**: Just `Scenario`, no complex hierarchies
3. **Pure generation**: No built-in evaluation or judging
4. **Local-first**: Default to Ollama, no API keys required
5. **Research-focused**: Reproducible, exportable, citable
6. **Progressive enhancement**: Data structures grow richer as they flow through the system

## What We DON'T Do

- âŒ Score or rank AI responses
- âŒ Maintain benchmark leaderboards
- âŒ Provide "correct" answers
- âŒ Judge safety automatically

## Integration Example

```
Your System
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Discord   â”‚â”€â”€â”€â”€â–¶â”‚scenario-forgeâ”‚â”€â”€â”€â”€â–¶â”‚ Your Safety  â”‚
â”‚     Bot     â”‚     â”‚  (generates) â”‚     â”‚  Evaluator   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²                                            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    Feedback Loop
```

## Training Pipeline Architecture

### Datastore Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SQLite Database                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Scenarios  â”‚       â”‚ ScenarioRatings  â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚ id          â”‚â”€â”€â”€â”€â”€â”€â”€â”‚ scenario_id      â”‚    â”‚
â”‚  â”‚ prompt      â”‚       â”‚ quality_score    â”‚    â”‚
â”‚  â”‚ target      â”‚       â”‚ rated_at         â”‚    â”‚
â”‚  â”‚ criteria    â”‚       â”‚ rated_by         â”‚    â”‚
â”‚  â”‚ created_at  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Rating Workflow

```
Generate â†’ Store â†’ Review â†’ Rate â†’ Export
    â”‚        â”‚        â”‚       â”‚       â”‚
    â–¼        â–¼        â–¼       â–¼       â–¼
Scenario  SQLite  CLI UI  Quality  Training
         Database         Score     Data
```

## The Strangeloop: scenario-forge + model-forge

scenario-forge is part of a larger ecosystem for AI safety evaluation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ scenario-forge  â”‚â”€â”€â”€â”€â”€â–¶â”‚ model-forge  â”‚â”€â”€â”€â”€â”€â–¶â”‚   CIRISAI   â”‚
â”‚   (generate)    â”‚      â”‚ (fine-tune)  â”‚      â”‚ (evaluate)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–²                                              â”‚
         â”‚                                              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    (improve scenarios)
```

### Integration with model-forge

`model-forge` (sister package) consumes scenarios to fine-tune evaluation models:

```python
# model-forge reads from scenario-forge's datastore
scenarios = ScenarioStore().get_high_quality_scenarios("ai_psychosis")
model = fine_tune_llama(scenarios)
export_to_gguf(model, "llama-psychosis-evaluator-v1")
```

This creates specialized evaluator models that can be deployed in CIRISAI for
consistent, domain-specific safety evaluation.

## Future Considerations

- Scenario templates for common domains
- Seed-based reproducibility
- Difficulty calibration
- Multi-turn scenario generation
- Active learning loops for model improvement
- Direct model-forge integration API

But always: **Generation only. Never evaluation.**
